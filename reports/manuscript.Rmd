---
title             : "False memories for scenes using DRM paradigm"
shorttitle        : "DRM for scenes"

author: 
  - name          : "Filip Děchtěrenko"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Hybernská 8, 11000, Prague 1"
    email         : "filip.dechterenko@gmail.com"
  - name          : "Jiří Lukavský"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Charles University"
  - id            : "2"
    institution   : "Insitite of Psychology, Czech Academy of Sciences"

authornote: |
  Filip Děchtěrenko, Department of Psychology, Faculty of Arts, Charles University, Celetná 20, 110 00, Prague, Czech Republic. Institute of Psychology, Czech Academy of Sciences, Hybernská 8, 110 00, Prague, Czech Republic. Jiří Lukavský, Institute of Psychology, Czech Academy of Sciences, Hybernská 8, 110 00, Prague, Czech Republic.

  Work of FD was supported by student grant no. 604218 nad by RVO 68081740. work JL was supported by GACR no. 16-07983S.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["d:/Dropbox/library.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_docx
---

```{r setup, include = FALSE}
library("papaja")
library(here)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(190813)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Experiment 1

Are humans able to understand the inner representation of the convolutional neural networks? In this experiment, we decided to validate, whether the similiarity space defined by convolutional neural network can be understood by human observers. We defined the similairty space by evaluating the images by pretrained  CNN [@zhou2014] and computing pairwise L2 distances for each images. Using this structure, we repeatedly selected clusters of similar 8 scenes, while manipulate the distance of the remaining ninth scene from the rest. Participants' task was to detect the odd-one scene. 

## Methods

### Participants
Twelve subjects (mean age = 24.83, SD = 3.49, 6 males) participated in this experiment. All of them had normal or corrected-to-normal vision and none of that had participated in this type of experiment before. Subjects were students of Faculty of Arts and participated as a part of a course requirements. All participants gave an informed consent before the experiment. sample size was based on prior power analysis done in software G*Power 3.1.92 [@Faul2009]. We were aiming to detect latge effect using one-sample t-tests while having $\alpha$=.05 and power $1-\beta$ = .8., which resulted in the abovementioned twelve participants.

### Stimuli and scene similarity

Scenes were selected from scene database [@konkle2010b]. From this database, we selected 64 categories having 32 photographs of scenes resulting in 2048 scenes in total. By using pre-trained CNN from @zhou2014, we created the multidimansional space as follows. Each image was evaluated by the CNN and we took output of the last fully inner connected layer (fc7) as the representation of the scene (vector of 4096 numbers). The distance metric in this structure was L2 norm. This resulted in a 4096 multidimensional image space. The scene categories were marked as either natural (12 categories) or man-manmade (52 categories).

### Design


For each semantic category in the databse, we created the experimental stimuli as follows. We randomly sample one scene (here denoted as _selected scene_). For this scene, we computed distances to all other scenes from the same category (31 distances). We divided the distances into 5 quintiles. As the first quintile would contain too similar scenes to the selected scene and last quintile could contain outliers with respect to typical exemplars in the category, we used only quintiles 2,3, and 4. Example of division of distances into quintile for one category is shown in Figure \@ref(fig:odd-hist-example).

(ref:odd-hist-example) Histogram of distances between selected scene and other scenes in one semantic category. The vertical lines shows division between quintiles.  


```{r odd-hist-example, fig.cap = "(ref:odd-hist-example)", fig.align = 'center', fig.width = 6}
knitr::include_graphics(here("plots","Fig_distance_histogram.png"))
```


For selected scene, we chose 7 closest scenes (_distractors_) and one scene selected from quintile 2-4 (_target_). The selection procedure is schematically visualized in Figure \@ref(fig:odd-scheme). The selection of quintile was randomized for each scene category (21-22 categories per quintile). 

(ref::odd-scheme) Schema of scene selection. Green dot denotes selected scene, seven blue dots denotes distractors, and red dot denotes target scene. The representation is visualized in 2D for simplicity.

```{r odd-scheme, fig.cap = "(ref:odd-scheme)", fig.align = 'center', fig.width = 6}
knitr::include_graphics(here("plots","Fig_odd_scheme.png"))
```

### Procedure

Experiment was programmed in software Psychopy1.90.2 [@Peirce2019]. There were 64 trials in total. In each trial, nine scenes were shown in a rectangular grid (size of each scene was 256$\times$256 px). The scenes were selected as described in Design section. The position of all scenes in the grid was randomized. The task was to detect the scene that was most different from the rest. No additional instructions, which criteria should be taken into account for oddity were not given. Participants responded by mouse selection, the time for the response was unlimited. The experiment lasted approximately 20 minutes. We created six versions of the protocols and each protocols was supplied to two partcipants.  

## Data analysis

Data were analyzed in statistical environment R 3.6 [@r2019]. For the analysis of differences in detection accuracy between quintiles were used generalized linear mixed models. The division into quintiles was used as a fixed factor, while participant id and category id as random factors. First, we used maximal models (both intercept and slopes) for random factors as suggested by [@Barr2013], but as the models lead to to the singualar fit, we reverted to the models with random intercept only. The significance of the fit was tested using likelihood ratio test. 

As the division into quintiles lose the information about the distance distribution, we performed additional analysis, in which we expressed the oddity of the target as ratio $\frac{d_{ST}}{d_{avg}}$, where $d_{ST}$ is distance between selected scene and the target and $d_{avg}$ is average distance of distractors to the target. Note that sole distance between target and selected scene would not represent the spread of the closest neighbours. for this structure, we again used generalized linear mixed models. Moreover, we tested the accuracy in detecting the odd scene from chance level (1/9) using one sample t-test. The effect size in that case was expressed using Cohen's _d_.

Due to the coding error, in several conditions (23 trials out of 768). there were used two same distractors. We replicated the analysis without those trials, but the results were similar. 

## Results

### Accuracy for detecting different scene
The accuracy of detecting different scene significantly increased with increased distance ($\chi^2(1) = 21.7, p < .001$). As visualized in Figure \@ref(fig:odd-results) The accuracy for second quintile was not significant from the chance (chance level: 1/9, mean = 10%, SD = 9 %, _t_(11) = -0.316, _p_ = .758, _d_ = 0.09), while they were significant for third quintile (mean = 21%, SD = 9 %, _t_(11) = 3.9, _p_ = .003, _d_ = 1.12) and fourth quintile (mean = 26%, SD = 12 %, _t_(11) = 4.35, _p_ = .001, _d_ = 1.26). When the task was reparametrized using ratio of distances, we achieved same results ($\chi^2(1) = 30.4, p < .001$) as visualized in right part of Figure \@ref(fig:odd-results). 
In Figure \@ref(fig:odd-results-category), we can see accuracy for each categogy. The exploration of the image space for top and bottom categories did not reveal any obvious pattern. In order to do that, we reduced the data using multidimensional scaling. The image space for top category (field) and bottom category (closet) is visualized in Figure \@ref(fig:odd-results-MDS).
As each pair of participants received same set of images, we tested, whether the participants make the same pattern when selecting odd scene. We estimated the agreement in selection using Cohen's $\kappa$, Average agreement was .24 (SD = .06), which is considered as weak agreement [@fleiss2003]. Participants therefore selected different scenes as the most different ones.

(ref::odd-results) Average accuracy when detecting odd scene. Left plot shows accuracy for each quintle. Dot shows average accuracy, while vertical lines shows bootstrapped standard error of the mean. The distribution of the data is shown by violin plot. Right part shows fitted generalized linear mixed model when the distance was expressed using ratio of distances. 

```{r odd-results, fig.cap = "(ref:odd-results)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_results.png"))

knitr::include_graphics(here("plots","Fig_odd_results.png"), here("plots","Fig_odd_results_ratio.png"))

```

(ref::odd-results-category) Average accuracy per each category. No clear trend for natural nad man-made scene was observed. Vertical lines denote bootstrapped standard error of the mean.

```{r odd-results-category, fig.cap = "(ref:odd-results-category)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_results_category.png"))

```

(ref::odd-results-MDS) Image space in MDS coordinates. Left shows category with lowest averace accuracy, right shows category with highest average accuracy.

```{r odd-results-MDS, fig.cap = "(ref:odd-results-MDS)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_results_MDS.png"))

```

### Exploratory analysis using different metrics

The similarity defined by image space is very complex. It is possible that as the instructions were very general ("detect the most different scene from the rest"), it is possible that participants used different strategies that can be explained by different metrics, which express the similarity in human-readable terms (such as differences in color). Additionally, we could see, whether we could explain the prediction from image space metric by simpler metrics. We selected three descriptors and computed L2 distance between the descriptors for pairs of scenes. 

We selected following descriptors: Gist descriptor [@oliva2001], which computes spatial envelope of the scene and is related to high-level properties of the scene, such as openess or naturalness; Histogram of oriented gradients (HOG), [@Dalal2005], which computes average orienation in small patches of the images and it is usually used for detecting objects; Pairwise RGB histogram comparison, which computes the color histograms and compares their differences; Scale invariant feature transform (SIFT), [@Lowe2004], which is also used for object detection. We selected these descriptors as they represent spatial layout of the scene (GIST), differences in color (RGB) and differences in objects (HOG, SIFT). Descriptors were computed in MATLAB 2018 using custom scripts.

Each of the descriptors was used as follows. For each 9 scenes, we computed pairwise L2 distances between corresponding descriptors resulting into distance matrix. From this structure, we evaluated the distances for each scene from the rest using heierachical clustering as suggested by [@Torgo2007]. This method ranked the scenes in amount, how much they are distant from the rest. This selection was then compared with human selection of odd scene and with selection of distant scene by image space metric.

Left part of Figure \@ref(fig:odd-metrics) shows, that none of the simpler metrics could predict selection of distant scene as defined by image space or predict human performance. For the case of predicting the image space, with increasing distance, the prediction by GIST increased, while for scenes close to the distractors, metrics used for object detection showed best performance in explaining image space properties. 

Inidividual metrics were not able to explain the human selection of distant scene, showing that decision strategies, which scene is most different is complex.

(ref::odd-metrics) Prediction of odd scene using gist, HOG, SIFT, and RGB metrics. Left plots shows prediction of odd scene as defined using image space structure. Right part shows prediction of human performance. 

```{r odd-metrics, fig.cap = "(ref:odd-metrics)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_metrics.png"))

```

## Discussion

Results showed that for distant scenes, participants were able to detect them highly above chance level (Cohen's _d_s >= 1.12). Moreover, the pattern showed increasing trend and with richer scene database, we could observe even higher accuracies. This result is surprising given the complexity of the task. For example, when discussing individual strategies for single category (bathroom), one of the participants replied, that he selected one scene bacause the bathroom was more european-like, while the others were more american-like. The scale of european-like/americanlike would be difficult to express using simpler decision criteria. Moreover, similar individual criteria could be different for each category or set of images. This was supported by evalution of agreement of subjects shoing low agreement. 

When trying to decompose the decision criteria into simpler ones using well-known image descriptors, we observed low accuracy showing that participants had indeed various strategies for detection. Neverthless, they were able to detect the distant scenes above chance level. 

In this experiment, we measured the odd scenes within semantic scene categories. We also tested three additional subjects in the experiment, in which the selection of distractors and target scene were not limited to one category only, but it was computed for all images in the database. The task then became trivial as the distant scene was always selected from different categories, while the distractors were from the same category. In that case, all participants reached perfect performance. 

The use of similarity space defined by CNN is widely used as possible model of scene representation [@Groen2018], however the inner structure of the similarity space is dificult to understand. Promising approach to decompose this structure is to create additional similarity spaces defined by simpler metrics and see, where the structures show resemblance and where do they differ. In the case of our image space, we showed signs that scenes close together shares similar spatial layout while the differences for a tight clusters of the scenes might be in the presence of different objects. The similairty with GISt was shown in our previous study with image space [@lukavsky2017]

Taken together, even with the small sample size, we shown that participants are able to understand the distances defined by the image space and thus this experiment serves as validation experiment for further use of image spaces. Similar approach could be used for other perceotual spaces, defined either by CNN e.g. [@Groen2018, @lukavsky2017] or by multidimensional scaling [@Hout2014a].

# Experiment 2

In the main experiment, we focused on the creation of false memories while using the properoties of image space defined by CNN. In particular, when memorizing large number of scenes [as in. @konkle2010b], how are the memories organised? Do we form a prototypical representation from the presented stimuli and compare the presented stimuli to this prototype? If the prototype is formed, two conditions should hold. First, the recognition for presented scenes closer to the prototype should be higher. Second, for the unpresented scenes close to the prototype, the false alarm rate should be higher. In this experiment, we focused on the latter condition.

We tested the memory performance in an paradigm inspired by Deese-Roedrigez-McDermott paradigm [DRM, @Deese1959, @Roediger1995]. We presented the scenes and participants task was to memorize then. The similairty defined by image space serves as valid tool how to quantify similarity of stimuli and describe the presented stimuli using the coordinates in image space, where the center of presented stimuli should lie. we queried teh subjects with either seen or unseen scenes, while we varied, whether the scenes were closer to the center of the presented scenes or further away. If the visual stimuli are internally represented in a structure showing resemblance to the image space, there should be more false alarms for the distractors closer to the center than for distractors further away from the 


## Methods

### Participants

Twenty-seven subjects participated in the experiment (mean age = 22.96, SD = 4.65, 5 males). All of them had normal or corrected-to-normal eyesight and none of them were colorblind. Twelve subjects partcipated in exchange for a course credit while remaining 15 participated for financial reward 200 Czech crowns (approximately 8 euros). All participants gave an informed consent before the experiment. Sample size was determined by prior power analysis. We needed 27 subjects for detection of medium effect (Cohen's d = 0.5) using one-sample t-test, $\alpha$=.05 and having power of test 1-$\beta$=0.8. we did not perform power analysis regarding linear mixed models, as there is no scientific consensus, how to proceed in that case. However, the sample size is typical for studies regarding visual perception.

### Simuli

We selected the stimuli for the experiment from the database FIGRIM [@bylinskii2015], which contains 21 categories, each with more than 300 scenes of size 700$\times$700 px.






\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
