---
title             : "False memories for scenes using DRM paradigm"
shorttitle        : "DRM for scenes"

author: 
  - name          : "Filip Děchtěrenko"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Hybernská 8, 11000, Prague 1"
    email         : "filip.dechterenko@gmail.com"
  - name          : "Jiří Lukavský"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Charles University"
  - id            : "2"
    institution   : "Insitite of Psychology, Czech Academy of Sciences"

authornote: |
  Filip Děchtěrenko, Department of Psychology, Faculty of Arts, Charles University, Celetná 20, 110 00, Prague, Czech Republic. Institute of Psychology, Czech Academy of Sciences, Hybernská 8, 110 00, Prague, Czech Republic. Jiří Lukavský, Institute of Psychology, Czech Academy of Sciences, Hybernská 8, 110 00, Prague, Czech Republic.

  Work of FD was supported by student grant no. 604218 and by RVO 68081740. work JL was supported by GACR no. 16-07983S.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["d:/Dropbox/library.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_docx
---

```{r setup, include = FALSE}
library("papaja")
library(here)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(190813)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

World around us is full of information to process and store. When styduing visual perception, it has been observed that visual memory has massive capacity for complex stimuli [@standing1970, @standing1973]. Recent studies showed that we are able to store not only shallow representation of the objects, but they are able to store many details [@brady2008] and similar findings were observed for scenes [@konkle2010b]. 
Although the capacity is large, it is an open question, how is it organised. The high recollection accuracy get lower when using yes/no task instead of 2AFC [@Cunningham2015]. Moreover, recent results show that the memory is much more limited in detail than it seems [@Draschkow2019].

How is the long-term memory orgranised? One possible explanation would be to create a coding scheme, using which the memories will be organised [@brady2011], similarly as in working memory [@brady2009]. This coding scheme is created during our lifetime with the formation of semantical categories [@Kemp2009]. In the case of objects, the possible candidate is a hierarchical structure from simple features through larger meaningful groups to whole objects [@brady2001, @Ullman2007]. In the case of scenes is the situation more complicated as not only the objects in the scene should be represented, but their spatial arrangment as well as the gist of the scene should be stored [@Oliva2005]. The organisation of whole scenes in the memory remains an open question.

One possible approach to this problem could be through formation of categories. The semantic categories in our mind are formed to match the categories of outside world and to be economic with terms of used properties to describe the structure [@rosch1978]. The category is formed around a prototype - a typical exemplar, which shows the maximal resemblance with rest of the members in the category, while having a low resemblance with other categories [@rosch1975b]. The prototypes are created not only for semantic categories, but also for visual stimuli. There are two main theories around visual prototype formation. The prototype-distance model [@Posner1970] states that the prototype is formed to minimize the distance from the presented stimuli, while the attribute-frequency model [@Neumann1974] states that prototype consist of attributes that were presented with highest frequency. Further studies showed higher experimental support for prototype-distance model for geometric patterns [@Franks1971] or faces [@Solso1979,@Solso1993,@Sampaio2018]. In the context of visual scenes, similar findings were not studied as it was difficult how to measure, what is a visual prototype. 

Recent years brought possible solution to problem of visual prototypesimilarity of scenes. @Oliva2001 created a GIST descriptor that characterizes the spatial layout of the scene and it can be abstracted to natural properties of the scenes [@Torralba2003], which human observers can naturally use [@greene2009, @greene2010]. For this descriptor, we could compute pairwise distances, which would allows us to determine, which allows us to create an similarity space. With advances in training the convolutional neural networks [CNN, @Lecun2015,@goodfellow2016], they CNNs were widely used for various task in vision science in areas like scene categorization [@Krizhevsky2012], object recognition [@Kheradpisheh2016] or memorability of the image [@khosla2015]. One particular use of CNNs is to use the pretrained network, present it with a stimuli and use activation in one of the layers as a descriptor of the stimuli. The descriptors extracted in this way are knows as *deep features*. Again, we can compute distance between the deep features using L2 norm. @lukavsky2017 used this approach in a memory study to explore the properties of the similarity space defined by deep features. 

In this study, we explore the properties of similarity space (which we denote as *image space*) defined by neural networks in two experiments. First, we explored whether the distances in the image space are understadable by human observers. Do humans understand, which scenes are closer to each other as defined by the image space? Second, we use the image space to verify, whether the visual prototypes are formed for complex scenes similarly as for simpler stimuli. 


# Experiment 1

Are humans able to understand the inner representation of the convolutional neural networks? In this experiment, we decided to validate, whether the similiarity space defined by convolutional neural network can be understood by human observers. We defined the similairty space by evaluating the images by pretrained  CNN [@zhou2014] and computing pairwise L2 distances for each images. Using this structure, we repeatedly selected clusters of similar 8 scenes, while manipulate the distance of the remaining ninth scene from the rest. Participants' task was to detect the odd-one scene. 

## Methods

### Participants
Twelve subjects (mean age = 24.83, SD = 3.49, 6 males) participated in this experiment. All of them had normal or corrected-to-normal vision and none of that had participated in this type of experiment before. Subjects were students of Faculty of Arts and participated as a part of a course requirements. All participants gave an informed consent before the experiment. sample size was based on prior power analysis done in software G*Power 3.1.92 [@Faul2009]. We were aiming to detect latge effect using one-sample t-tests while having $\alpha$=.05 and power $1-\beta$ = .8., which resulted in the abovementioned twelve participants.

### Stimuli and scene similarity

Scenes were selected from scene database [@konkle2010b]. From this database, we selected 64 categories having 32 photographs of scenes resulting in 2048 scenes in total. By using pre-trained CNN from @zhou2014, we created the multidimansional space as follows. Each image was evaluated by the CNN and we took output of the last fully inner connected layer (fc7) as the representation of the scene (vector of 4096 numbers). The distance metric in this structure was L2 norm. This resulted in a 4096 multidimensional image space. The scene categories were marked as either natural (12 categories) or man-manmade (52 categories).

### Design


For each semantic category in the databse, we created the experimental stimuli as follows. We randomly sample one scene (here denoted as _selected scene_). For this scene, we computed distances to all other scenes from the same category (31 distances). We divided the distances into 5 quintiles. As the first quintile would contain too similar scenes to the selected scene and last quintile could contain outliers with respect to typical exemplars in the category, we used only quintiles 2,3, and 4. Example of division of distances into quintile for one category is shown in Figure \@ref(fig:odd-hist-example).

(ref:odd-hist-example) Histogram of distances between selected scene and other scenes in one semantic category. The vertical lines shows division between quintiles.  


```{r odd-hist-example, fig.cap = "(ref:odd-hist-example)", fig.align = 'center', fig.width = 6}
knitr::include_graphics(here("plots","Fig_distance_histogram.png"))
```


For selected scene, we chose 7 closest scenes (_distractors_) and one scene selected from quintile 2-4 (_target_). The selection procedure is schematically visualized in Figure \@ref(fig:odd-scheme). The selection of quintile was randomized for each scene category (21-22 categories per quintile). 

(ref::odd-scheme) Schema of scene selection. Green dot denotes selected scene, seven blue dots denotes distractors, and red dot denotes target scene. The representation is visualized in 2D for simplicity.

```{r odd-scheme, fig.cap = "(ref:odd-scheme)", fig.align = 'center', fig.width = 6}
knitr::include_graphics(here("plots","Fig_odd_scheme.png"))
```

### Procedure

Experiment was programmed in software Psychopy1.90.2 [@Peirce2019]. There were 64 trials in total. In each trial, nine scenes were shown in a rectangular grid (size of each scene was 256$\times$256 px). The scenes were selected as described in Design section. The position of all scenes in the grid was randomized. The task was to detect the scene that was most different from the rest. No additional instructions, which criteria should be taken into account for oddity were not given. Participants responded by mouse selection, the time for the response was unlimited. The experiment lasted approximately 20 minutes. We created six versions of the protocols and each protocols was supplied to two partcipants.  

## Data analysis

Data were analyzed in statistical environment R 3.6 [@r2019]. For the analysis of differences in detection accuracy between quintiles were used generalized linear mixed models. The division into quintiles was used as a fixed factor, while participant id and category id as random factors. First, we used maximal models (both intercept and slopes) for random factors as suggested by [@Barr2013], but as the models lead to to the singualar fit, we reverted to the models with random intercept only. The significance of the fit was tested using likelihood ratio test. 

As the division into quintiles lose the information about the distance distribution, we performed additional analysis, in which we expressed the oddity of the target as ratio $\frac{d_{ST}}{d_{avg}}$, where $d_{ST}$ is distance between selected scene and the target and $d_{avg}$ is average distance of distractors to the target. Note that sole distance between target and selected scene would not represent the spread of the closest neighbours. for this structure, we again used generalized linear mixed models. Moreover, we tested the accuracy in detecting the odd scene from chance level (1/9) using one sample t-test. The effect size in that case was expressed using Cohen's _d_.

Due to the coding error, in several conditions (23 trials out of 768). there were used two same distractors. We replicated the analysis without those trials, but the results were similar. 

## Results

### Accuracy for detecting different scene
The accuracy of detecting different scene significantly increased with increased distance ($\chi^2(1) = 21.7, p < .001$). As visualized in Figure \@ref(fig:odd-results) The accuracy for second quintile was not significant from the chance (chance level: 1/9, mean = 10%, SD = 9 %, _t_(11) = -0.316, _p_ = .758, _d_ = 0.09), while they were significant for third quintile (mean = 21%, SD = 9 %, _t_(11) = 3.9, _p_ = .003, _d_ = 1.12) and fourth quintile (mean = 26%, SD = 12 %, _t_(11) = 4.35, _p_ = .001, _d_ = 1.26). When the task was reparametrized using ratio of distances, we achieved same results ($\chi^2(1) = 30.4, p < .001$) as visualized in right part of Figure \@ref(fig:odd-results). 
In Figure \@ref(fig:odd-results-category), we can see accuracy for each categogy. The exploration of the image space for top and bottom categories did not reveal any obvious pattern. In order to do that, we reduced the data using multidimensional scaling. The image space for top category (field) and bottom category (closet) is visualized in Figure \@ref(fig:odd-results-MDS).
As each pair of participants received same set of images, we tested, whether the participants make the same pattern when selecting odd scene. We estimated the agreement in selection using Cohen's $\kappa$, Average agreement was .24 (SD = .06), which is considered as weak agreement [@fleiss2003]. Participants therefore selected different scenes as the most different ones.

(ref::odd-results) Average accuracy when detecting odd scene. Left plot shows accuracy for each quintle. Dot shows average accuracy, while vertical lines shows bootstrapped standard error of the mean. The distribution of the data is shown by violin plot. Right part shows fitted generalized linear mixed model when the distance was expressed using ratio of distances. 

```{r odd-results, fig.cap = "(ref:odd-results)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_results.png"))

knitr::include_graphics(c(here("plots","Fig_odd_results.png"), here("plots","Fig_odd_results_ratio.png")))

```

(ref::odd-results-category) Average accuracy per each category. No clear trend for natural nad man-made scene was observed. Vertical lines denote bootstrapped standard error of the mean.

```{r odd-results-category, fig.cap = "(ref:odd-results-category)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_results_category.png"))

```

(ref::odd-results-MDS) Image space in MDS coordinates. Left shows category with lowest averace accuracy, right shows category with highest average accuracy.

```{r odd-results-MDS, fig.cap = "(ref:odd-results-MDS)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_results_MDS.png"))

```

### Exploratory analysis using different metrics

The similarity defined by image space is very complex. It is possible that as the instructions were very general ("detect the most different scene from the rest"), it is possible that participants used different strategies that can be explained by different metrics, which express the similarity in human-readable terms (such as differences in color). Additionally, we could see, whether we could explain the prediction from image space metric by simpler metrics. We selected three descriptors and computed L2 distance between the descriptors for pairs of scenes. 

We selected following descriptors: Gist descriptor [@oliva2001], which computes spatial envelope of the scene and is related to high-level properties of the scene, such as openess or naturalness; Histogram of oriented gradients (HOG), [@Dalal2005], which computes average orienation in small patches of the images and it is usually used for detecting objects; Pairwise RGB histogram comparison, which computes the color histograms and compares their differences; Scale invariant feature transform (SIFT), [@Lowe2004], which is also used for object detection. We selected these descriptors as they represent spatial layout of the scene (GIST), differences in color (RGB) and differences in objects (HOG, SIFT). Descriptors were computed in MATLAB 2018 using custom scripts.

Each of the descriptors was used as follows. For each 9 scenes, we computed pairwise L2 distances between corresponding descriptors resulting into distance matrix. From this structure, we evaluated the distances for each scene from the rest using heierachical clustering as suggested by [@Torgo2007]. This method ranked the scenes in amount, how much they are distant from the rest. This selection was then compared with human selection of odd scene and with selection of distant scene by image space metric.

Left part of Figure \@ref(fig:odd-metrics) shows, that none of the simpler metrics could predict selection of distant scene as defined by image space or predict human performance. For the case of predicting the image space, with increasing distance, the prediction by GIST increased, while for scenes close to the distractors, metrics used for object detection showed best performance in explaining image space properties. 

Inidividual metrics were not able to explain the human selection of distant scene, showing that decision strategies, which scene is most different is complex.

(ref::odd-metrics) Prediction of odd scene using gist, HOG, SIFT, and RGB metrics. Left plots shows prediction of odd scene as defined using image space structure. Right part shows prediction of human performance. 

```{r odd-metrics, fig.cap = "(ref:odd-metrics)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_odd_metrics.png"))

```

## Discussion

Results showed that for distant scenes, participants were able to detect them highly above chance level (Cohen's _d_s >= 1.12). Moreover, the pattern showed increasing trend and with richer scene database, we could observe even higher accuracies. This result is surprising given the complexity of the task. For example, when discussing individual strategies for single category (bathroom), one of the participants replied, that he selected one scene bacause the bathroom was more european-like, while the others were more american-like. The scale of european-like/americanlike would be difficult to express using simpler decision criteria. Moreover, similar individual criteria could be different for each category or set of images. This was supported by evalution of agreement of subjects shoing low agreement. 

When trying to decompose the decision criteria into simpler ones using well-known image descriptors, we observed low accuracy showing that participants had indeed various strategies for detection. Neverthless, they were able to detect the distant scenes above chance level. 

In this experiment, we measured the odd scenes within semantic scene categories. We also tested three additional subjects in the experiment, in which the selection of distractors and target scene were not limited to one category only, but it was computed for all images in the database. The task then became trivial as the distant scene was always selected from different categories, while the distractors were from the same category. In that case, all participants reached perfect performance. 

The use of similarity space defined by CNN is widely used as possible model of scene representation [@Groen2018], however the inner structure of the similarity space is dificult to understand. Promising approach to decompose this structure is to create additional similarity spaces defined by simpler metrics and see, where the structures show resemblance and where do they differ. In the case of our image space, we showed signs that scenes close together shares similar spatial layout while the differences for a tight clusters of the scenes might be in the presence of different objects. The similairty with GISt was shown in our previous study with image space [@lukavsky2017]

Taken together, even with the small sample size, we shown that participants are able to understand the distances defined by the image space and thus this experiment serves as validation experiment for further use of image spaces. Similar approach could be used for other perceotual spaces, defined either by CNN e.g. [@Groen2018, @lukavsky2017] or by multidimensional scaling [@Hout2014a].

# Experiment 2

In the main experiment, we focused on the creation of false memories while using the properoties of image space defined by CNN. In particular, when memorizing large number of scenes [as in. @konkle2010b], how are the memories organised? Do we form a prototypical representation from the presented stimuli and compare the presented stimuli to this prototype? If the prototype is formed, two conditions should hold. First, the recognition for presented scenes closer to the prototype should be higher. Second, for the unpresented scenes close to the prototype, the false alarm rate should be higher. In this experiment, we focused on the latter condition.

We tested the memory performance in an paradigm inspired by Deese-Roedrigez-McDermott paradigm [DRM, @Deese1959, @Roediger1995]. We presented the scenes and participants task was to memorize then. The similairty defined by image space serves as valid tool how to quantify similarity of stimuli and describe the presented stimuli using the coordinates in image space, where the center of presented stimuli should lie. we queried teh subjects with either seen or unseen scenes, while we varied, whether the scenes were closer to the center of the presented scenes or further away. If the visual stimuli are internally represented in a structure showing resemblance to the image space, there should be more false alarms for the distractors closer to the center than for distractors further away from the center, as the center represent the visual prototype [@Posner1969].


## Methods

### Participants

Twenty-seven subjects participated in the experiment (mean age = 22.96, SD = 4.65, 5 males). All of them had normal or corrected-to-normal eyesight and none of them were colorblind. Twelve subjects partcipated in exchange for a course credit while remaining 15 participated for financial reward 200 Czech crowns (approximately 8 euros). All participants gave an informed consent before the experiment. Sample size was determined by prior power analysis. We needed 27 subjects for detection of medium effect (Cohen's d = 0.5) using one-sample t-test, $\alpha$=.05 and having power of test 1-$\beta$=0.8. we did not perform power analysis regarding linear mixed models, as there is no scientific consensus, how to proceed in that case. However, the sample size is typical for studies regarding visual perception.

### Simuli

We selected the stimuli for the experiment from the database FIGRIM [@bylinskii2015], which contains 21 categories, each with more than 300 scenes of size 700$\times$700 px. Photographs of scenes in FIGRIM database come from larger SUN database [@xiao2010], which contain many artifacts (e.g. some scenes were not photographs of real scenes, but they were built from lego bricks, contain camera recording markers, etc.). Before the selection of the stimuli, we manually removed are scenes that had some artifacts in them. After the selection, 8665 photographs remained. For those scenes, we again evaluated each of them by pre-trained CNN and created an image space from the activation of fc7 layer.

### Design

whole design is visualized in Figure \@ref(fig:drm-scheme).For each scene category, we first computed the category center. then we divided the scenes by the distances from the center into groups closer than median and farther than median. From the closer group, we randomly selected 15 scenes, which were used in training block. These 15 scenes create a polygon in the image space and therefore we could compute the its center representing average target scene. From the group of closer scenes, we select 5 more scenes closest to the center of the training scenes. These scenes will de denoted as *close-distractors*. From the group of farther scenes, we select randomly 5 scenes, which will be denoted as *far-distractors*. Finnaly, from the training scenes, we selected five which will be presented in testing block. 
Each category was sampled three times, while unique scenes were used for each set.

(ref::drm-scheme) Schema of experimental design. The black x denotes category center and dotted circle group of closer scenes. For randomly selected targets from the closer group (green dots), we compute the center (green x) and select close-distractors (red dots). From the farther scenes, we select far-distractors (blue dots). 

```{r drm-scheme, fig.cap = "(ref:drm-scheme)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_scheme.png"))

```

### Procedure

Experiment was programmed in PsychoPy. Participants were seated approximately 60 cm from the LCD display (22") Experiment consisted alternating parts consisting of training blocks (15 training scenes) and testing blocks (15 scenes). From the 15 testing scenes, five were previously presented target scenes, five were close-distractors and remaining five were far-distractors. Before each training scene, fixation cross was shown for 300 ms. Before the first scene in experimental training block, name of the category was shown instead. In the testing part, participants responded using keyboard. First, they responded using arrow, whether the scene was presented or not. Left arrow denoted presented scene, while right arrow denoted novel scene. Then they rated their confidence on scale "not sure at all", "somewhat sure", and "very sure". The selection of the confidence was again done using arrow keys (down arrow confirmed the selection)-Whole procedure is visualized in Figure \@ref(fig:drm-procedure).
Participants were instructed that the speed of answer is not important, just the accuracy. There were no breaks in the experiment, but participants were instructed to take breaks when needed. Experiment lasted 50-70 minutes.

(ref::drm-procedure) Experimental procedure for one training and testing blog. 

```{r drm-procedure, fig.cap = "(ref:drm-procedure)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_procedure.png"))

```

### Data analysis

Analysis was again performed in statistical tool R. The differences between three types of scenes (target, close-distractor, far-distractor), with focus on differences between far- and close- distractors, were tested using generalized linear mixed models. Participant id and category id as random factors while keeping the model maximal (both intercept and slope). Similarly as in Experiment 1, we tested the differences against chance level (50%).

We also analyzed the data using Signal detection theory approach [@green1966,@MacmillanNA2004]. Use of confidence ratings allowed us to construct empirical ROC curves with nonunit slopes for discriminating between targets and close-distractors and targets and far-distractors. We expressed the performance using $d_a$ measure as an alternative to traditional $d'$ used in a case of non-unit ROC slopes, which is recommened to be use in memory studies [@Loiotile2015]. Using this approach, the bias is not computed traditionally, but instead the the perceptual distance between scene types is computed [@MacmillanNA2004, @Loiotile2015]. 

## Results

The recognition accuracy for target scenes was 69% (SD = 12%) as visualized in Figure \@ref(fig:drm-results-main). In the case of distractors, the incorrect answer represent a false alarm. The false alarm rate for close distractors was 24% (SD = 17%), while for far-distractors the false alarm rate was only 11%.

One sample t-tests revealed that recognition was above chance level in all three conditions (_p_s < .001, Cohen's _d_s > 1.52). The difference between two distractor types was sinigficant ($\chi^2(1) = 56.1, _p_ < .001$)

(ref::drm-results-main) Accuracy results for each type of scene. The shape of data distribution is visualized using violin plot. The vertical line shows bootstrapped standard error of the mean, horizontal line denotes chance level. The false alarm rate for distractors could be computed by subtracting the accuracy from 100%.  

```{r drm-results-main, fig.cap = "(ref:drm-results-main)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_main.png"))

```

By using the confidence data taken together with old/new answers, we obtained empirical ROC curves (visualized in Figure \@ref(fig:drm-results-ROC)) for both discrimination conditions. Both curves had slope lesser then 1 (targets vs close distractors: slope = 0.62, SD = 0.14; targets vs far distractors: slope = 0.75, SD = 0.23) and the difference from unit slope was significant (_p_s < .001), so we had to use index $d_a$ for measuring sensitivity. Discriminability between target scenes and close distractors was 1.15 (SD = 0.56) and between target scenes and far distractors was 1.71 (SD = 0.61). The difference in sensitivity was significant (paired t-test: t(25) = -14.33, _p_ < .001, _d_ = 2.81). When visualizing the schema SDT model (Figure \@ref(fig:drm-results-SDT-model)), the perceptual distance between distractors [@Dosher1984] was small (pseudo $d_a$ = 0.56). 


(ref::drm-results-ROC) ROC curves for discrimination between Target and close-distractors (left) and target and far-distractors (right). Black line denotes chance level. Red line shows average ROC curve, while the grey lines shows ROC curves for individual participants. 

```{r drm-results-ROC, fig.cap = "(ref:drm-results-ROC)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_ROC.png"))

```

(ref::drm-results-SDT-model) SDT model for three scene types. The perceptual distance between distractors was small (pseudo $d_a$ = 0.56). 

```{r drm-results-SDT-model, fig.cap = "(ref:drm-results-SDT-model)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_SDT_model.png"))

```

The exploration of recognition accuracy per each scene category revealed that easiest were categories bridge and bathroom, while most difficult were badlands and cockpit. Full results are displayed in Figure \@ref(fig:drm-results-category). On the level individual scene, we explored, whether the memory accuracy could be explained by memorability of the photograph [@Isola2001]. The memorability is good candidate to predict the task, because it has been shown that memorable images shows good perceptual organisation [@Goetschalckx2018] and are quickly categorisable [@Broers2017]. Therefore, we evaluated the photographs using trained CNN LaMem, which can predict memorability similarly to the behavioral scores [@khosla2015]. We then computed correlation of memory accuracy with this predicted memorability. The results showed that the correlation was small and significant for targets (_r_ = .10, _p_ = .003)) and for close-distractors(_r_ = .15, _p_ = .007)), while not significant for far-distractors (_r_ = .05, _p_ = .335). Therefore, the effect of memorability is only small. 

(ref::drm-results-category) Moving average for reaction time (size of window was 200 consescutive trials) with respect to id of the trial. 

```{r drm-results-category, fig.cap = "(ref:drm-results-category)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_categories.png"))

```

We were also interested, whether the recognition of the presented scenes propagated to the time needed to answer. We excluded the responses longer than 5s. Participants tended to make faster responses during the time course of the experiment as visualized in Figure \@ref(fig:drm-results-rt_window). As visualized in Figure \@ref(fig:drm-results-rt), the fastest responses were for far-distractors (mean = 1.09 s, SD = 0.29), followed by target scenes (mean = 1.19 s, SD = 0.21), while slowest responses were for close-distractors (mean = 1.26 s, SD = 0.21). The differences were significant ($\chi^2(1) = 27.8, _p_ < .001$). Pairwise post-hoc analysis with Tukey's correction revealed that the difference in reaction times between targets and close-distractors were not significant (_p_ = .256), while the remaing pairwise differences were signifcant (p < .001). Moreover, the difference in reaction times between close- and far-distractors was large (Cohen's d = 1.57).

(ref::drm-results-rt-window) Moving average for reaction time (size of window was 200 consescutive trials) with respect to id of the trial. 

```{r drm-results-rt-window, fig.cap = "(ref:drm-results-rt-window)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_rt_window.png"))

```

(ref::drm-results-rt) Reaction times for each scene type. The shape of data distribution is visualized using violin plot. The vertical line shows bootstrapped standard error of the mean.

```{r drm-results-rt, fig.cap = "(ref:drm-results-rt)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_rt.png"))

```

Because the experiment was long and potentially tireing, we analyzed the accuracy using moving average similarly as for reaction times. As visualized in Figure \@ref(fig:drm-results-window-acc), the accuracy remained stable for distreactors and declined for targets after approximately 1000th trial. However, the critical difference between distractors was stable across the whole trial.


(ref::drm-results-window-acc) Moving average (N = 200) for recognition accuracy. Accuracy for the distractors was stable, while the accuracy declined approximately after 1000th trial. 

```{r drm-results-window-acc, fig.cap = "(ref:drm-results-window-acc)", fig.align = 'center'}
knitr::include_graphics(here("plots","Fig_drm_results_rt.png"))

```

## Discussion

In this experiment, we explored, whether we form a perceptual prototype when memorizing complex stimuli such as scenes. We found out that distractors closer to the average of the presented scenes show higher false alarm rate than distractors far from the average. Similar results were observed for simpler stimuli such as geometric stimuli [@solso1979] or simple faces [@Solso1981], not for complex scenes. 

Here presented evidence about the prototypical formation of scene average is not definite proof about use of image space as a model of memory for scenes. Recent literature however shows that in a terms of scene classification, deep features show high resemblance to functional connectivity [@Groen2018]. Moreever the presented approach could be easily extended to compare between several competing models. We could select stimuli in a similar manner using different image spaces and see, which structure would show more false alarms.  

Analysis of reaction times showed that scenes closer to the visual prototype required more time to respond. This is in line with findings regarding reaction time in prototype formation for numerical prototypes [@Solso1993] or for scene classification using adjectives [@greene2009]. 

One problem with image space that as the representation is highly dimensional, it is very densly populated even for large scene databases. It limits finer experimental manipulation in order to create actual visual prototype. One possible solution to this problem would be to use either 3d models of the scene or take advantage of recent advances in artificial scenes created by Generative adversial networks [@goodfellow2014] which were recently prototyped for scene creation based on the semantic description [@park2019].  

Our reserach complements findings from scene categorization, which show similar resemblance to scene memory. Research focused on scene categorization showed converging evidence that categorizing scenes by their function is best model explaining human performance and it outperforms deep features [@greene2015]. However on the neural level, the functional connectivity appears as better model showing higher resemblance with behavioral data [@Groen2018]. Similar findings could be observed in a case of visual memory for the scenes. 

# Conclusion

In these two experiments, we explored the possible use of convolutional neural networks as a possible model for representation of the memory. We observed that participants intuitively understand the distances in the image space (Experiment 1) and when we preselect stimuli based on the their distance as estimated by deep features, we can observe typical effects in formation of prototypes. Taken together with our past research using deep features [@lukavsky2017], we believe that measure of perceptual similarity as defined by convolutional neural networks shows promising result as a valid metric.

# Acknowledgement

The work of F.D. on this research was supported by a student grant GAUK No. 604218 and by Grant No. RVO 68081740. The work of J.L. was part of a research program of the Czech Academy of Sciences, Strategy AV21, and was supported by Czech Science Grant Foundation No. 16-07983S.

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
